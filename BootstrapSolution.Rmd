---
Title: "Bootstrap Example"
author: "D. Zeitler"
output: html_notebook
---

Bootstrapping
================

## Task 1: Create an RMarkdown document

Done.

## Task 2: Load the necessary packages

```{r Setup}
library(ggformula)
library(tidyverse)
library(tidymodels)
```

## Task 3: Create the data

To help conceptualize bootstrapping to traditional methods that we
explore earlier this semester, we will create our own dataset. This way,
we will know the truth about the population from which we are drawing
data and can compare how bootstrapping and the traditional methods
compare (and are different).

Note that in practice, you will likely never know the population. Or, if
you have the population information, there is no need to do any
inference (again, you already know the population). Therefore, in
practice for doing bootstrapping, you would simply start with something
like Task 5 of this activity.

- Create a new R code chunk and type the following code:

```{r Configure}
# Set a random seed value so we can obtain the same "random" results
set.seed(2025)

n <- 2000   # number of records of data

```


```{r DataGeneration}
# Create a data frame/tibble named sim_dat
sim_dat <- tibble(
  # Get n uniform random numbers between -5 and 5 in x1
  x1 = runif(n, -5, 5),
  # Get n uniform random numbers between 0 and 100 in x2
  x2 = runif(n, 0, 100),
  # Get n binomial (0,1) random numbers x3 with probability of 1 being 1/2
  x3 = rbinom(n, 1, 0.5)
)

# Set values for model parameters beta 0 through beta3 and sigma
b0 <- 2
b1 <- 0.25
b2 <- -0.5
b3 <- 1
sigma <- 1.5

# These are the random values, the epsilons, normal zero mean and standard devation sigma
errors <- rnorm(n, 0, sigma)

# Generate the response 'y' in the data frame and convert x3 to character
sim_dat <- sim_dat %>% 
  mutate(
    y = b0 + b1*x1 + b2*x2 + b3*x3 + errors,
    x3 = case_when(
      x3 == 0 ~ "No",
      TRUE ~ "Yes"
    )
  )
```

Complete the following tasks:
1.  Done
2.  The true model is $y= 2 + 0.25 x1 - 0.5 x2 + x3 + \epsilon$
3.  Create graphical visualizations for the relationship between all
    variable *pair*s (i.e., `y` and each `x` and also each `x` pair).
    Provide a brief summary of what you see/notice. That is, how do
    these relationships compare with your comments from (1) and model in
    (2)? Especially for the relationship between `y` and each `x`. Hint:
    Do you remember what function/package makes this very easy to
    produce?

```{r message=FALSE}
library(GGally)
ggpairs(sim_dat)
```

## Task 4: Traditional MLR model

First we will fit an estimated model to our simulated data. Recall that
we have done some similar work in past activities, but for ease of
searching I will tell you what to do.

```{r MLR}
mlr_fit <- linear_reg() %>%
  set_mode("regression") %>% 
  set_engine("lm") %>% 
  fit(y ~ x1 + x2 + x3, data = sim_dat)

# Also include the confidence intervals for our estimated slope parameters
tidy(mlr_fit, conf.int = TRUE)
```

Answer the following question:

4.  Looking at your population-level model from (2), how accurate are
    your results? Explain how you made this decision. That is, what did
    you use from your output and how did you use that information to
    decide?
Look at the estimate, conf.low/high and compare to b0(2), b1(.25), b2(-.5) and b3(1). Note how wide the intervals are. Why?

## Task 5: Bootstrapping

Now, bootstrapping treats your sample of data as a psuedo-population
that you will create multiple new samples from. Each sample will be of
the same size and same number of variables as the original
($n = 20; p = 3$). However, once a row has been sampled, it will be put
back into the “population” (i.e., sampling with replacement). With
computing power being relatively cheap now, we can do this for a large
number of times to build up $B$ bootstrap samples. For example, we might
do $B = 2,000$ bootstrap samples each of $n = 20$.

```{r Bootstrap}
# Set a random seed value so we can obtain the same "random" results
set.seed(631)

# Generate the 2000 bootstrap samples
boot_samps <- sim_dat %>% 
  bootstraps(times = 2000)

boot_samps
```

When viewing this outputted object, it probably looks a little odd. This
is a nested data set with two columns:

- `splits`: An `rsplit` object that has two main components: an analysis
  dataset and an assessment dataset.
- `id`: A label of which bootstrap sample it is.

View the first analysis dataset

```{r AnalysisSample1}
boot_samps$splits[[1]] %>% analysis()
```

And the first assessment dataset

```{r AssessmentSample1}
boot_samps$splits[[1]] %>% assessment()
```

For today, we will only focus on the analysis datasets.

Now we need to fit a linear model to each bootstrap sample. Hopefully,
from STA 518, you remember some of the handy iteration functions from
`{purrr}`.

```{r FitBootstrapSets}


# Create a function that fits a fixed MLR model to one split dataset
fit_mlr_boots <- function(split) {
  lm(y ~ x1 + x2 + x3, data = analysis(split))
}

# Fit the model to each split and store the information
# Also, obtain the tidy model information
boot_models <- boot_samps %>% 
  mutate(
    model = map(splits, fit_mlr_boots),
    coef_info = map(model, tidy)
  )

boots_coefs <- boot_models %>% 
  unnest(coef_info)

boots_coefs
```

We can then calculate the bootstrap intervals by obtaining the
$2.5^{th}$ and $97.5^{th}$ percentiles - similar to a 95% confidence
interval as we are finding the values that contain the middle 95% of the
bootstrap values. Note that we provide the level of significance (1 -
confidence level): $\alpha = 0.05 = 1 - 0.95$.

```{r BootstrapIntervals}
boot_int <- int_pctl(boot_models, statistics = coef_info, alpha = 0.05)
boot_int
```

Visualize this information to get a sense of the variability of the estimates.

```{r VisualizeIntervals}
ggplot(boots_coefs, aes(x = estimate)) +
  geom_histogram(bins = 30) +
  facet_wrap( ~ term, scales = "free") +
  geom_vline(data = boot_int, aes(xintercept = .lower), col = "blue") +
  geom_vline(data = boot_int, aes(xintercept = .upper), col = "blue")
```

Answer the following question:

5.  Looking at your population-level model from (2), how accurate are
    your results? Explain how you made this decision. That is, what did
    you use from your output and how did you use that information to
    decide?

### Challenge

In your previous code chunk, add the following:

- A pair of vertical orange lines that correspond to the traditional
  method’s 95% confidence intervals
- A single vertical blue line that corresponds to the population slope
  value (there should be a unique blue line in each plot)

Once you can verify these “simpler” asks, dress your plot up. Use colors
that speak to you. Use themes that speak to you. Make this plot yours!

## What is next?

We will explore cross validation methods.

## Task 6: Leave-One-Out Cross Validation (LOOCV)

The LOOCV estimate can be automatically computed for any generalized linear
model using the `glm()` and `cv.glm()` functions.  We have used the `glm()`
function to perform logistic regression by passing in  the `family = "binomial"`
argument. But if we use `glm()` to fit a model without passing in the `family`
argument, then it  performs linear regression, just like the `lm()` function.
So for instance,

```{r}
glm(y ~ ., data = sim_dat) %>% coef()
```

and

```{r}
lm(y ~ ., data = sim_dat) %>% coef()
```

 yield identical linear regression models. Here we will  perform linear regression using
 the `glm()` function rather than the `lm()` function because the former can be used together with
`cv.glm()`. The `cv.glm()` function is part of the `boot` library.

```{r chunk8}
library(boot)
lr <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("glm")

glm.fit <- lr %>% fit(y ~ ., data = sim_dat)
cv.err <- cv.glm(sim_dat, extract_fit_engine(glm.fit))
cv.err$delta
```

The `cv.glm()` function produces a list with several components.  The two numbers in the `delta` vector contain the cross-validation results. In this case the numbers are very close and correspond to the LOOCV statistic given in (5.1).  The first is the standard $k$-fold CV estimate, as in (5.3). The second is a bias-corrected version.

Try LOOCV with tidy models.
```{r}
glm.folds <- vfold_cv(sim_dat,v=n)
glm.folds
```

```{r}
lm_wf <-
  workflow() %>%
  add_model(lr) %>%
  add_formula(y~.)

lm_fit_rs <- 
  lm_wf %>%
  fit_resamples(glm.folds)
```

Doesn't work. Stick with the glm/glm.cv for LOOCV.

## Task 7: $k$-Fold Cross Validation

Let's do a k-fold CV on the simulated data.

```{r}
# Use parallel processing?
library(doMC)
detectCores()
registerDoMC(cores=6)

lm_spec <- linear_reg() %>%
  set_engine("lm")

lm_wf <-
  workflow() %>%
  add_model(lm_spec) %>%
  add_formula(y~.)

lm.folds <- vfold_cv(sim_dat,v=10)

lm_fit_rs <- 
  lm_wf %>%
  fit_resamples(
    resamples = lm.folds,
    control = control_resamples(extract = extract_model, save_pred = TRUE))

lm_fit_rs  
```


```{r}
collect_metrics(lm_fit_rs)
```


```{r}
assess_res <- collect_predictions(lm_fit_rs)
assess_res
```

```{r}
assess_res %>%
  gf_point(.pred~y,col=~id) %>%
  gf_abline(slope = 1)
```

```{r}
lm_fit_rs$.extracts %>% 
  .[1:3]
```

```{r}
# Let's unnest this and get the coefficients out
model_coefs <- lm_fit_rs %>% 
  select(id, .extracts) %>%                    # get the id and .extracts columns
  unnest(cols = .extracts) %>%
  mutate(coefs = map(.extracts, tidy)) %>%     # use map() to apply the tidy function and get the coefficients in their own column
  unnest(coefs)                                # unnest the coefs column you just made to get the coefficients for each fold
 
model_coefs
```

```{r}
## Plot the model coefficients and 2*SE across all folds
model_coefs %>%
  filter(term != "(Intercept)") %>%
  select(id, term, estimate, std.error) %>%
  group_by(term) %>%
  mutate(avg_estimate = mean(estimate)) %>%
  ggplot(aes(x = id, y = estimate)) +
  geom_hline(aes(yintercept = avg_estimate),
             size = 1.2,
             linetype = "dashed") +
  geom_point(size = 4) +
  geom_errorbar(aes(ymin = estimate - 2*std.error, ymax = estimate + 2*std.error),
                width = 0.1,
                size = 1.2) +
  facet_wrap(~term, scales = "free_y") +
  labs(x = "CV Folds",
       y = "Estimate ± 95% CI",
       title = "Regression Coefficients ± 95% CI for 10-fold CV",
       subtitle = "Dashed Line = Average Coefficient Estimate over 10 CV Folds per Independent Variable") +
  theme_classic() +
  theme(strip.background = element_rect(fill = "black"),
        strip.text = element_text(face = "bold", size = 12, color = "white"),
        axis.title = element_text(size = 14, face = "bold"),
        axis.text.x = element_text(angle = 60, hjust = 1, face = "bold", size = 12),
        axis.text.y = element_text(face = "bold", size = 12),
        plot.title = element_text(size = 18),
        plot.subtitle = element_text(size = 16))
```


